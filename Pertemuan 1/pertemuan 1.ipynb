{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pertemuan 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Python**\n",
    "\n",
    "Python akan digunakan sebagai bahasa pemrograman dalam praktikum **Pemrosesan Bahasa Alami** ini. Untuk memastikan kelancaran praktikum, pastikan Python sudah terinstal di perangkat masing-masing. Disarankan untuk menggunakan versi Python *3.9* atau yang lebih baru, karena versi ini mendukung pustaka dan fitur terbaru yang dibutuhkan dalam pemrosesan bahasa alami. Jika Python belum terinstal, Anda dapat mengunduhnya dari situs resmi Python dan mengikuti petunjuk instalasi sesuai dengan sistem operasi yang digunakan.\n",
    "\n",
    "<h3>Why Python ?</h3>\n",
    "\n",
    "Python dipilih untuk praktikum ini karena beberapa alasan berikut:\n",
    "- Kemudahan Penggunaan: Sintaksis Python yang sederhana dan mudah dipahami memungkinkan penerapan berbagai teknik pemrosesan bahasa alami (NLP).\n",
    "- Ekosistem Pustaka yang Kuat: Python memiliki banyak pustaka seperti NLTK, spaCy, dan scikit-learn yang sangat mendukung pengolahan teks. Pustaka-pustaka ini mempermudah implementasi tugas-tugas NLP, seperti tokenisasi, analisis sentimen, dan ekstraksi entitas.\n",
    "- Komunitas yang Aktif: Python memiliki komunitas besar yang aktif berbagi pengetahuan, solusi, dan sumber daya pembelajaran.\n",
    "- Kecepatan Pengembangan: Python memungkinkan pengembangan yang cepat berkat sintaks yang mudah dan pustaka yang sudah teroptimasi.\n",
    "- Dukungan untuk Pembelajaran Mesin: Python menawarkan berbagai algoritma pembelajaran mesin yang sudah siap pakai dan dioptimalkan untuk NLP.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"img/python-libraries-ACTE.png\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Collecting**\n",
    "\n",
    "Selanjutnya, kita akan memasuki tahap berikutnya, yaitu mempelajari dan mendemonstrasikan berbagai metode pengumpulan data dalam NLP. Pengumpulan data merupakan langkah awal yang penting, karena kualitas serta variasi data akan mempengaruhi hasil analisis dan pengembangan model. Pada bagian ini, kita akan mengeksplorasi berbagai sumber data dan teknik yang dapat diterapkan untuk mengumpulkan informasi yang diperlukan dalam pemrosesan bahasa alami."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mengambil Data dari X menggunakan tweepy\n",
    "\n",
    "Twitter menyimpan sejumlah besar data yang penuh dengan informasi berharga. Para pemasar media sosial memanfaatkan data ini untuk mendukung strategi mereka. Setiap hari, jutaan tweet diposting, masing-masing menyimpan cerita unik. Ketika data tersebut dikumpulkan dan dianalisis, hal ini dapat memberikan wawasan yang sangat berguna bagi perusahaan mengenai produk, layanan, dan berbagai aspek operasional mereka.\n",
    "\n",
    "Pada bagian ini, kita akan mempelajari cara mengakses dan mengambil data dari Twitter menggunakan pustaka `tweepy`. Dengan `tweepy`, kita dapat terhubung ke API Twitter dan mengumpulkan tweet sesuai dengan kriteria yang ditentukan. Berikut adalah demonstrasi mengenai cara menggunakan `tweepy` dalam kode program untuk mengambil data dari Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tweepy in /home/rin1412/.local/lib/python3.13/site-packages (4.15.0)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in /home/rin1412/.local/lib/python3.13/site-packages (from tweepy) (3.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in /usr/lib/python3.13/site-packages (from tweepy) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib<3,>=1.2.0 in /home/rin1412/.local/lib/python3.13/site-packages (from tweepy) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/lib/python3.13/site-packages (from requests<3,>=2.27.0->tweepy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.13/site-packages (from requests<3,>=2.27.0->tweepy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3.13/site-packages (from requests<3,>=2.27.0->tweepy) (1.26.20)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Instalasi library tweepy\n",
    "%pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengimpor pustaka-pustaka yang diperlukan\n",
    "import numpy as np\n",
    "import json\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "\n",
    "# Kredensial API\n",
    "consumer_key = \"your_consumer_key\"\n",
    "consumer_secret = \"your_consumer_secret\"\n",
    "access_token = \"your_access_token\"\n",
    "access_token_secret = \"your_access_token_secret\"\n",
    "bearer_token = 'your_bearer_token'\n",
    "\n",
    "# Membuat client API Twitter V2\n",
    "client = tweepy.Client(bearer_token=bearer_token, consumer_key=consumer_key, consumer_secret=consumer_secret, access_token=access_token, access_token_secret=access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk menarik tweet berdasarkan kata kunci tertentu, kita dapat menggunakan kode berikut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @Dukesno1: @Granite77777 é€™å°±æ˜¯æœ‰çµ„ç¹”çš„è¼¿è«–æˆ°å…§å®¹ï¼Œæ‡‰å°éœ€è¦æ˜ç¢ºå¹¾å€‹äº‹å¯¦ï¼š\n",
      "1ã€NIHè³‡åŠ©çš„æ˜¯é˜²æ­¢ç—…æ¯’å¤§æµè¡Œé …ç›®ã€Baricæ•™æˆçš„æ˜¯ç”Ÿç‰©é†«å­¸æŠ€è¡“ï¼Œè¢«ä¸­å…±è»äº‹é†«å­¸ç§‘å­¸é™¢ã€æ­¦æ¼¢ç—…æ¯’æ‰€åˆ©ç”¨è£½é€ Covid19ç—…æ¯’\n",
      "2ã€ä¸­å…±å¾2019å¹´12æœˆåˆè‡³2020å¹´1â€¦\n",
      "RT @JamesSentacnoly: @uaRM3CDQBGDIJm7 ã‚³ãƒ­ãƒŠã‚¦ã‚¤ãƒ«ã‚¹ã¯ç›®ã‹ã‚‰ã‚‚æ„ŸæŸ“ã—ã€é€†ã«è¡€ç®¡ã€ç¥çµŒã‹ã‚‰ç›®ã«ã‚‚åˆ°é”ã—ã€éšœå®³ã‚’èµ·ã“ã—ã¾ã™\n",
      "ç¶²è†œè¡€ç®¡ã¯å¿ƒè¡€ç®¡ã®æŠ¹æ¶ˆã€€çœ¼ç§‘çš„ç—‡çŠ¶ã¨ã®é–¢é€£\n",
      "https://t.co/wmq80VgSgP\n",
      "ç›®ã‹ã‚‰ã®æ„ŸæŸ“ã ã‘ã§ã¯ãªãä½“æ¶²â€¦\n",
      "RT @Incorrectibles: ğŸ”´ğŸ’¥ LES RÃ‰VÃ‰LATIONS EXPLOSIVES DE PIERRE CHAILLOT (@decoder_l) SUR LE #COVID19 !\n",
      "\n",
      "Â«Â Il nâ€™y a pas la moindre Ã©tude scientâ€¦\n",
      "RT @sarahmarsham: @SadiqKhan @RichardWatts01 @CarolineLucas @CarolineRussell @jeremycorbyn @TheGreenParty  @IslingtonLabour @Clive_Carter @â€¦\n",
      "RT @Incorrectibles: ğŸ”´ğŸ’¥ LES RÃ‰VÃ‰LATIONS EXPLOSIVES DE PIERRE CHAILLOT (@decoder_l) SUR LE #COVID19 !\n",
      "\n",
      "Â«Â Il nâ€™y a pas la moindre Ã©tude scientâ€¦\n",
      "RT @SenNenadiParody: She walking free.\n",
      "\n",
      "No charges.\n",
      "\n",
      "Legend of our democracy\n",
      "\n",
      "Sadiya Farouq, Frm Humanitarian Disaster.\n",
      "\n",
      "â‚¦37.1bn Gone\n",
      "\n",
      "N500â€¦\n",
      "@bysisa_isah @MarioNawfal No, because they create solutions looking for a problem, and by doing it they create problems, such as covid19, or overblow problems like climate change and then create additional problems solving the created problem inflation and potentially energy and food scarcity.\n",
      "RT @SenNenadiParody: She walking free.\n",
      "\n",
      "No charges.\n",
      "\n",
      "Legend of our democracy\n",
      "\n",
      "Sadiya Farouq, Frm Humanitarian Disaster.\n",
      "\n",
      "â‚¦37.1bn Gone\n",
      "\n",
      "N500â€¦\n",
      "RT @pravinK29417382: Donald trump is posted in public as hero of america,more narratives are coming in sequence because democrate are moreâ€¦\n",
      "RT @AranazJoseju: \"Diario de una #JMJ (XIX)\":\n",
      "\n",
      "https://t.co/9dmA0b6IFh\n",
      "\n",
      "#COVID19 #COVIDãƒ¼19 #Covid #COVID_19 #Rusia #Ucrania #Russia #Israelâ€¦\n",
      "RT @AranazJoseju: \"Diario de una #JMJ (XIX)\":\n",
      "\n",
      "https://t.co/9dmA0b6aPJ\n",
      "\n",
      "#COVID19 #COVIDãƒ¼19 #Covid #COVID_19 #Rusia #Ucrania #Russia #Israelâ€¦\n",
      "RT @AranazJoseju: \"Diario de una #JMJ (XIX)\":\n",
      "\n",
      "https://t.co/9dmA0b6aPJ\n",
      "\n",
      "#COVID19 #COVIDãƒ¼19 #Covid #COVID_19 #Rusia #Ucrania #Russia #Israelâ€¦\n",
      "@HendrikWuest Auch Covid19 verkÃ¼rzt das Leben.\n",
      "Aber das verleugnet man ja.\n",
      "RT @uniumigame: ãã†ã ã£ãŸã®ã‹ã€ã€ã‚¤ãƒ¼ãƒ­ãƒ³ãƒã‚¹ã‚¯\n",
      "\n",
      "#COVID19\n",
      "#ã‚³ãƒ­ãƒŠãƒ¯ã‚¯ãƒãƒ³\n",
      "\n",
      "#USAIDScandal \n",
      "#SNSã®è¨€è«–çµ±åˆ¶ã«åå¯¾ã—ã¾ã™ \n",
      "\n",
      "https://t.co/bMJPNKObzU\n",
      "RT @BumblebeeJoe: Zo. Dat u nog maar eens helder voor ogen heeft wat instanties als het @rivm, het NIVEL en het @Lareb_NL bedoelen als zijâ€¦\n",
      "RT @BumblebeeJoe: Zo. Dat u nog maar eens helder voor ogen heeft wat instanties als het @rivm, het NIVEL en het @Lareb_NL bedoelen als zijâ€¦\n",
      "RT @robinmonotti: BlackRock CEO explains how the real goal of depopulation (Covid19, Midazolam, Remdesivir, \"vaccines\", euthanasia, abortioâ€¦\n",
      "RT @MILTONRIANOP: Las abejas son parte fundamental de nuestro planeta y parte de la vida del ser humano, lo cual estamos acabando sus colmeâ€¦\n",
      "ãã‚ŒãŒã€Œç£ã®åˆ»å°ã€ï¼Covid19 ãƒ¯ã‚¯ãƒãƒ³ã§ã—ãŸï½—ğŸ¤£ https://t.co/ayW8AaFoLC\n",
      "@BGatesIsaPyscho What a farce.\n",
      "Are politicians  taking HIV test in public the new COVID19 stunt ?\n"
     ]
    }
   ],
   "source": [
    "result = client.search_recent_tweets(query=\"covid19\", max_results=20)\n",
    "for tweet in result.data:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penjelasan Kode:\n",
    "- Kode di atas akan menarik 10 tweet teratas saat kata kunci 'ABC' dicari.\n",
    "- API ini akan mengambil tweet dalam bahasa Indonesia (dengan lang='id'), dan mengecualikan tweet yang merupakan retweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Membaca Data dari PDF\n",
    "\n",
    "Pada bagian ini, kita akan mempelajari cara mengambil teks dari file PDF, sehingga informasi yang ada di dalamnya dapat digunakan untuk analisis lebih lanjut dalam pemrosesan bahasa alami.\n",
    "\n",
    "Demonstrasi berikut akan memanfaatkan pustaka Python `PyPDF2` untuk mengekstraksi teks dari file PDF. Dengan menggunakan `PyPDF2`, kita dapat dengan mudah mengakses konten dokumen PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pertama, install terlebih dahulu pustaka `PyPDF2` dan mengimpornya dalam kode program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: PyPDF2 in /home/rin1412/.local/lib/python3.13/site-packages (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Instalasi library PyPDF2\n",
    "%pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya, Gunakan kode berikut untuk membuka file PDF, membaca isinya, dan mengekstrak teks dari halaman pertama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PyPDF2._page._VirtualList object at 0x7f604d5d0590>\n",
      "Modul 1 Praktikum Natural Language Processing\n",
      "Modul 1 Praktikum Natural Language Processing\n",
      "Pengenalan Pengolahan Data Teks menggunakan Python\n",
      "Deskripsi Singkat\n",
      "Modul ini akan membahas pengenalan pengolahan data teks menggunakan ba-\n",
      "hasa Python. Modul ini terdiri dari 5 materi yang akan membahas pengumpu-\n",
      "lan data menggunakan API, membaca data dari berbagai format file yang ada\n",
      "seperti PDF, Word, JSON, dan HTML.\n",
      "Tujuan\n",
      "1. Dapat mengumpulkan data menggunakan melalui API yang tersedia di\n",
      "internet.\n",
      "2. Dapat membaca data dari berbagai format file yang ada seperti PDF,\n",
      "Word, JSON, dan HTML.\n",
      "Materi 1 - Pengumpulan Data\n",
      "Studi Kasus\n",
      "Anda ingin mengumpulkan data menggunakan API Twitter\n",
      "Langkah-langkah\n",
      "1. Mendapatkan Bearer Token pada halaman Portal Developer Twitter\n",
      "https://developer.x.com/en/portal/\n",
      "2. Menginstall library tweepy\n",
      "pip install tweepy\n",
      "3. Import library yang dibutuhkan\n",
      "1import tweepy\n",
      "4. Memasukkan bearer token sebagai akses token ke API Twitter\n",
      "1consumer_key = \" your_consumer_key \"\n",
      "2consumer_secret = \" your_consumer_secret \"\n",
      "3access_token = \" your_access_token \"\n",
      "4access_token_secret = \" your_access_token_secret \"\n",
      "5bearer_token = â€™ your_bearer_token â€™\n",
      "5. Membuat client ke API Twitter\n",
      "1client = tweepy . Client ( bearer_token = bearer_token , consumer_key\n",
      "= consumer_key , consumer_secret = consumer_secret ,\n",
      "access_token = access_token , access_token_secret =\n",
      "access_token_secret )\n",
      "Prof. Dr. Taufik Fuadi Abidin, S.Si., M. Tech.\n",
      "Jurusan Informatika Universitas Syiah Kuala\n",
      "Modul oleh: Diky Wahyudi, Furqan Al Ghifari ZulvaPage 1\n"
     ]
    }
   ],
   "source": [
    "# Membuka file PDF\n",
    "pdf = open(\"pertemuan_1.pdf\", \"rb\")\n",
    "\n",
    "# Membuat objek pembaca PDF\n",
    "pdf_reader = PyPDF2.PdfReader(pdf)\n",
    "\n",
    "# Mengecek jumlah halaman dalam file PDF\n",
    "print(pdf_reader.pages)\n",
    "\n",
    "# Membuat objek halaman\n",
    "page = pdf_reader.pages[0]\n",
    "\n",
    "# Mengekstrak teks dari halaman\n",
    "print(page.extract_text())\n",
    "\n",
    "# Menutup file PDF setelah selesai\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penjelasan Kode:\n",
    "- Pertama, file PDF dibuka dalam mode rb (read-binary).\n",
    "- Objek `PdfFileReader` digunakan untuk membaca file PDF dan mengecek jumlah halaman dengan `pdf_reader.numPages`.\n",
    "- Untuk mengekstrak teks, kita menggunakan metode `extractText()` pada objek halaman.\n",
    "- Terakhir, file PDF ditutup setelah teks berhasil diambil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Membaca Data dari file Word (Microsoft Word)\n",
    "\n",
    "Pada bagian ini, kita akan mempelajari cara membaca data dari file Microsoft Word. File Word sering digunakan untuk menyimpan dokumen teks yang berisi informasi yang bisa dianalisis lebih lanjut dalam pemrosesan bahasa alami. Dengan menggunakan pustaka Python, kita dapat mengekstrak teks dari file Word dan memanfaatkannya untuk berbagai tujuan analisis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anda perlu menginstal pustaka python-docx dan mengimpornya:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-docx in /home/rin1412/.local/lib/python3.13/site-packages (1.1.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /usr/lib64/python3.13/site-packages (from python-docx) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /home/rin1412/.local/lib/python3.13/site-packages (from python-docx) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Instalasi pustaka python-docx\n",
    "%pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengimpor pustaka yang diperlukan\n",
    "import docx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setelah pustaka diimpor, Anda dapat membuka file Word dan mengekstrak teksnya:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aktifitas 1: (Menuju tur 360)\n",
      "Pertanyaan: Bagaimana pengguna menuju halaman tur 360?\n",
      "Uraian: Pengguna bisa menuju halaman tur 360.\n",
      "Langkah-langkah: \n",
      "Pengguna meng-klik tombol pada beranda.\n",
      "Klik tombol pada NavBar.\n",
      "Scroll halaman ke bawah.\n",
      "Data yang dikumpulkan:\n",
      "Komentar tampilan dengan menanyakan diakhir pengujian.\n",
      "Bisa melakukan aktifitas (sukses/gagal).\n",
      "Waktu melakukan aktifitas pakai stopwatch sampai selesai.\n",
      "\n",
      "Aktifitas 2: (Mengganti bahasa)\n",
      "Pertanyaan: Apakah pengguna mengetahui bahwa situs tersebut bisa menggunakan multi language?\n",
      "Uraian: Pengguna mengetahui situs memiliki fungsi multi language.\n",
      "Langkah-langkah: \n",
      "Pengguna meng-klik tombol di pojok kanan atas pada NavBar.\n",
      "Pilih bahasa yang diinginkan (ID/EN)\n",
      "Data yang dikumpulkan:\n",
      "Komentar tampilan dengan menanyakan diakhir pengujian.\n",
      "Bisa melakukan aktifitas (sukses/gagal).\n",
      "Waktu melakukan aktifitas pakai stopwatch sampai selesai.\n",
      "\n",
      "Aktifitas 3: (Menggunakan halaman tur 360)\n",
      "Pertanyaan: Bagaimana cara menggunakan halaman tur 360?\n",
      "Uraian: Pengguna bisa menggunakan halaman tur 360.\n",
      "Langkah-langkah: \n",
      "Pengguna meng-klik salah satu menu tur 360.\n",
      "Klik tombol fullscreen di kanan atas untuk membuat layar penuh.\n",
      "Click and Drop untuk melihat sekitar.\n",
      "Klik tombol lingkaran putih untuk menuju scene selanjutnya.\n",
      "Klik pada preview gambar di bawah untuk menuju scene yang diinginkan.\n",
      "Data yang dikumpulkan:\n",
      "Komentar tampilan dengan menanyakan diakhir pengujian.\n",
      "Bisa melakukan aktifitas (sukses/gagal).\n",
      "Waktu melakukan aktifitas pakai stopwatch sampai selesai.\n",
      "\n",
      "Aktifitas 4: (Menggunakan tombol pada tur 360)\n",
      "Pertanyaan: Apakah pengguna mengetahui fungsi dari tombol yang ada di tur 360?\n",
      "Uraian: Pengguna mengetahui tiap fungsi tombol pada halaman tur 360.\n",
      "Langkah-langkah: \n",
      "Pengguna meng-klik tombol lingkaran untuk menuju ke scene selanjutnya.\n",
      "Pengguna meng-klik tombol â€œiâ€ untuk memunculkan informasi (pada scene yang terdapat tombol â€œiâ€).\n",
      "Pengguna meng-klik tombol â€œ3Dâ€ untuk memunculkan 3D model artefak batu nisan (pada scene yang terdapat tombol â€œ3Dâ€).\n",
      "Data yang dikumpulkan:\n",
      "Komentar tampilan dengan menanyakan diakhir pengujian.\n",
      "Bisa melakukan aktifitas (sukses/gagal).\n",
      "Waktu melakukan aktifitas pakai stopwatch sampai selesai.\n",
      "\n",
      "Aktifitas 5: (Memunculkan informasi pada scene tur 360)\n",
      "Pertanyaan: Bagaimana pengguna memunculkan informasi di halaman tur 360?\n",
      "Uraian: Pengguna mengetahui cara menggunakan fitur informasi di halaman tur 360.\n",
      "Langkah-langkah: \n",
      "Pengguna meng-klik tombol â€œiâ€ untuk memunculkan informasi (pada scene yang terdapat tombol â€œiâ€).\n",
      "Scroll halaman ke bawah untuk menampilkan informasi secara keseluruhan.\n",
      "Klik tombol silang â€Xâ€ untuk menutup tampilan informasi.\n",
      "Data yang dikumpulkan:\n",
      "Komentar tampilan dengan menanyakan diakhir pengujian.\n",
      "Bisa melakukan aktifitas (sukses/gagal).\n",
      "Waktu melakukan aktifitas pakai stopwatch sampai selesai.\n",
      "\n",
      "Aktifitas 6: (Menggunakan halaman 3D model artefak batu nisan)\n",
      "Pertanyaan: Bagaimana pengguna menggunakan fitur 3D model artefak batu nisan di halaman tur 360?\n",
      "Uraian: Pengguna mengetahui cara menggunakan fitur 3D model artefak batu nisan di halaman tur 360.\n",
      "Langkah-langkah: \n",
      "Pengguna meng-klik tombol â€œ3Dâ€ untuk memunculkan halaman 3D model (pada scene yang terdapat tombol â€œ3Dâ€).\n",
      "Arahkan kursor menggunakan mouse atau touchpad untuk rotasi 3D model artefak batu nisan secara manual\n",
      "Scroll tombol tengah pada mouse untuk zoom in dan zoom out 3D model artefak batu nisan.\n",
      "Klik tombol fullscreen di kanan bawah untuk membuat layar penuh.\n",
      "Pengguna meng-klik tombol â€œiâ€ untuk memunculkan informasi terkait 3D model artefak batu nisan.\n",
      "Pengguna meng-klik tombol â€œauto rotationâ€ untuk rotasi 3D model secara otomatis.\n",
      "Pengguna meng-klik tombol â€œdisable textâ€ untuk menyembunyikan seluruh teks dan tombol di dalam widget.\n",
      "â€˜Qâ€™ dan â€˜Eâ€™ untuk menurunkan dan menaikkan 3D model artefak batu nisan.\n",
      "â€˜Sâ€™ untuk mengembalikan rotasi 3D model artefak batu nisan ke posisi awal.\n",
      "â€˜Wâ€™ untuk mengembalikan zoom objek batu nisan ke posisi awal.\n",
      "Data yang dikumpulkan:\n",
      "Komentar tampilan dengan menanyakan diakhir pengujian.\n",
      "Bisa melakukan aktifitas (sukses/gagal).\n",
      "Waktu melakukan aktifitas pakai stopwatch sampai selesai.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Membuka file Word\n",
    "doc = docx.Document(\"contoh-docx.docx\")\n",
    "\n",
    "# Membuat string kosong untuk menampung hasil ekstraksi\n",
    "docu = \"\"\n",
    "\n",
    "# Melakukan iterasi pada setiap paragraf dalam dokumen\n",
    "for para in doc.paragraphs:\n",
    "    docu += para.text + \"\\n\"  # Menambahkan setiap paragraf ke string\n",
    "\n",
    "# Menampilkan hasil ekstraksi teks\n",
    "print(docu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penjelasan Kode:\n",
    "- `Document(\"file.docx\")`: Membuka file `.docx`.\n",
    "- `for para in doc.paragraphs`: Melakukan iterasi pada setiap paragraf dalam dokumen Word.\n",
    "- `docu += para.text`: Menambahkan teks setiap paragraf ke dalam string docu.\n",
    "- `print(docu)`: Menampilkan hasil ekstraksi teks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Membaca Data dari file JSON\n",
    "\n",
    "Pada bagian ini, kita akan mempelajari cara membaca data dari file JSON. JSON sering digunakan untuk menyimpan data terstruktur dalam format teks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anda perlu menginstal pustaka yang diperlukan dan mengimpor pustaka tersebut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in /usr/lib/python3.13/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/lib/python3.13/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.13/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3.13/site-packages (from requests) (1.26.20)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Instalasi library yang diperlukan\n",
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengimpor pustaka\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kemudian, kita akan mengambil data dari file JSON menggunakan pustaka `requests` dan kemudian menampilkan hasilnya:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'userId': 1, 'id': 1, 'title': 'delectus aut autem', 'completed': False}, {'userId': 1, 'id': 2, 'title': 'quis ut nam facilis et officia qui', 'completed': False}, {'userId': 1, 'id': 3, 'title': 'fugiat veniam minus', 'completed': False}, {'userId': 1, 'id': 4, 'title': 'et porro tempora', 'completed': True}, {'userId': 1, 'id': 5, 'title': 'laboriosam mollitia et enim quasi adipisci quia provident illum', 'completed': False}, {'userId': 1, 'id': 6, 'title': 'qui ullam ratione quibusdam voluptatem quia omnis', 'completed': False}, {'userId': 1, 'id': 7, 'title': 'illo expedita consequatur quia in', 'completed': False}, {'userId': 1, 'id': 8, 'title': 'quo adipisci enim quam ut ab', 'completed': True}, {'userId': 1, 'id': 9, 'title': 'molestiae perspiciatis ipsa', 'completed': False}, {'userId': 1, 'id': 10, 'title': 'illo est ratione doloremque quia maiores aut', 'completed': True}, {'userId': 1, 'id': 11, 'title': 'vero rerum temporibus dolor', 'completed': True}, {'userId': 1, 'id': 12, 'title': 'ipsa repellendus fugit nisi', 'completed': True}, {'userId': 1, 'id': 13, 'title': 'et doloremque nulla', 'completed': False}, {'userId': 1, 'id': 14, 'title': 'repellendus sunt dolores architecto voluptatum', 'completed': True}, {'userId': 1, 'id': 15, 'title': 'ab voluptatum amet voluptas', 'completed': True}, {'userId': 1, 'id': 16, 'title': 'accusamus eos facilis sint et aut voluptatem', 'completed': True}, {'userId': 1, 'id': 17, 'title': 'quo laboriosam deleniti aut qui', 'completed': True}, {'userId': 1, 'id': 18, 'title': 'dolorum est consequatur ea mollitia in culpa', 'completed': False}, {'userId': 1, 'id': 19, 'title': 'molestiae ipsa aut voluptatibus pariatur dolor nihil', 'completed': True}, {'userId': 1, 'id': 20, 'title': 'ullam nobis libero sapiente ad optio sint', 'completed': True}, {'userId': 2, 'id': 21, 'title': 'suscipit repellat esse quibusdam voluptatem incidunt', 'completed': False}, {'userId': 2, 'id': 22, 'title': 'distinctio vitae autem nihil ut molestias quo', 'completed': True}, {'userId': 2, 'id': 23, 'title': 'et itaque necessitatibus maxime molestiae qui quas velit', 'completed': False}, {'userId': 2, 'id': 24, 'title': 'adipisci non ad dicta qui amet quaerat doloribus ea', 'completed': False}, {'userId': 2, 'id': 25, 'title': 'voluptas quo tenetur perspiciatis explicabo natus', 'completed': True}, {'userId': 2, 'id': 26, 'title': 'aliquam aut quasi', 'completed': True}, {'userId': 2, 'id': 27, 'title': 'veritatis pariatur delectus', 'completed': True}, {'userId': 2, 'id': 28, 'title': 'nesciunt totam sit blanditiis sit', 'completed': False}, {'userId': 2, 'id': 29, 'title': 'laborum aut in quam', 'completed': False}, {'userId': 2, 'id': 30, 'title': 'nemo perspiciatis repellat ut dolor libero commodi blanditiis omnis', 'completed': True}, {'userId': 2, 'id': 31, 'title': 'repudiandae totam in est sint facere fuga', 'completed': False}, {'userId': 2, 'id': 32, 'title': 'earum doloribus ea doloremque quis', 'completed': False}, {'userId': 2, 'id': 33, 'title': 'sint sit aut vero', 'completed': False}, {'userId': 2, 'id': 34, 'title': 'porro aut necessitatibus eaque distinctio', 'completed': False}, {'userId': 2, 'id': 35, 'title': 'repellendus veritatis molestias dicta incidunt', 'completed': True}, {'userId': 2, 'id': 36, 'title': 'excepturi deleniti adipisci voluptatem et neque optio illum ad', 'completed': True}, {'userId': 2, 'id': 37, 'title': 'sunt cum tempora', 'completed': False}, {'userId': 2, 'id': 38, 'title': 'totam quia non', 'completed': False}, {'userId': 2, 'id': 39, 'title': 'doloremque quibusdam asperiores libero corrupti illum qui omnis', 'completed': False}, {'userId': 2, 'id': 40, 'title': 'totam atque quo nesciunt', 'completed': True}, {'userId': 3, 'id': 41, 'title': 'aliquid amet impedit consequatur aspernatur placeat eaque fugiat suscipit', 'completed': False}, {'userId': 3, 'id': 42, 'title': 'rerum perferendis error quia ut eveniet', 'completed': False}, {'userId': 3, 'id': 43, 'title': 'tempore ut sint quis recusandae', 'completed': True}, {'userId': 3, 'id': 44, 'title': 'cum debitis quis accusamus doloremque ipsa natus sapiente omnis', 'completed': True}, {'userId': 3, 'id': 45, 'title': 'velit soluta adipisci molestias reiciendis harum', 'completed': False}, {'userId': 3, 'id': 46, 'title': 'vel voluptatem repellat nihil placeat corporis', 'completed': False}, {'userId': 3, 'id': 47, 'title': 'nam qui rerum fugiat accusamus', 'completed': False}, {'userId': 3, 'id': 48, 'title': 'sit reprehenderit omnis quia', 'completed': False}, {'userId': 3, 'id': 49, 'title': 'ut necessitatibus aut maiores debitis officia blanditiis velit et', 'completed': False}, {'userId': 3, 'id': 50, 'title': 'cupiditate necessitatibus ullam aut quis dolor voluptate', 'completed': True}, {'userId': 3, 'id': 51, 'title': 'distinctio exercitationem ab doloribus', 'completed': False}, {'userId': 3, 'id': 52, 'title': 'nesciunt dolorum quis recusandae ad pariatur ratione', 'completed': False}, {'userId': 3, 'id': 53, 'title': 'qui labore est occaecati recusandae aliquid quam', 'completed': False}, {'userId': 3, 'id': 54, 'title': 'quis et est ut voluptate quam dolor', 'completed': True}, {'userId': 3, 'id': 55, 'title': 'voluptatum omnis minima qui occaecati provident nulla voluptatem ratione', 'completed': True}, {'userId': 3, 'id': 56, 'title': 'deleniti ea temporibus enim', 'completed': True}, {'userId': 3, 'id': 57, 'title': 'pariatur et magnam ea doloribus similique voluptatem rerum quia', 'completed': False}, {'userId': 3, 'id': 58, 'title': 'est dicta totam qui explicabo doloribus qui dignissimos', 'completed': False}, {'userId': 3, 'id': 59, 'title': 'perspiciatis velit id laborum placeat iusto et aliquam odio', 'completed': False}, {'userId': 3, 'id': 60, 'title': 'et sequi qui architecto ut adipisci', 'completed': True}, {'userId': 4, 'id': 61, 'title': 'odit optio omnis qui sunt', 'completed': True}, {'userId': 4, 'id': 62, 'title': 'et placeat et tempore aspernatur sint numquam', 'completed': False}, {'userId': 4, 'id': 63, 'title': 'doloremque aut dolores quidem fuga qui nulla', 'completed': True}, {'userId': 4, 'id': 64, 'title': 'voluptas consequatur qui ut quia magnam nemo esse', 'completed': False}, {'userId': 4, 'id': 65, 'title': 'fugiat pariatur ratione ut asperiores necessitatibus magni', 'completed': False}, {'userId': 4, 'id': 66, 'title': 'rerum eum molestias autem voluptatum sit optio', 'completed': False}, {'userId': 4, 'id': 67, 'title': 'quia voluptatibus voluptatem quos similique maiores repellat', 'completed': False}, {'userId': 4, 'id': 68, 'title': 'aut id perspiciatis voluptatem iusto', 'completed': False}, {'userId': 4, 'id': 69, 'title': 'doloribus sint dolorum ab adipisci itaque dignissimos aliquam suscipit', 'completed': False}, {'userId': 4, 'id': 70, 'title': 'ut sequi accusantium et mollitia delectus sunt', 'completed': False}, {'userId': 4, 'id': 71, 'title': 'aut velit saepe ullam', 'completed': False}, {'userId': 4, 'id': 72, 'title': 'praesentium facilis facere quis harum voluptatibus voluptatem eum', 'completed': False}, {'userId': 4, 'id': 73, 'title': 'sint amet quia totam corporis qui exercitationem commodi', 'completed': True}, {'userId': 4, 'id': 74, 'title': 'expedita tempore nobis eveniet laborum maiores', 'completed': False}, {'userId': 4, 'id': 75, 'title': 'occaecati adipisci est possimus totam', 'completed': False}, {'userId': 4, 'id': 76, 'title': 'sequi dolorem sed', 'completed': True}, {'userId': 4, 'id': 77, 'title': 'maiores aut nesciunt delectus exercitationem vel assumenda eligendi at', 'completed': False}, {'userId': 4, 'id': 78, 'title': 'reiciendis est magnam amet nemo iste recusandae impedit quaerat', 'completed': False}, {'userId': 4, 'id': 79, 'title': 'eum ipsa maxime ut', 'completed': True}, {'userId': 4, 'id': 80, 'title': 'tempore molestias dolores rerum sequi voluptates ipsum consequatur', 'completed': True}, {'userId': 5, 'id': 81, 'title': 'suscipit qui totam', 'completed': True}, {'userId': 5, 'id': 82, 'title': 'voluptates eum voluptas et dicta', 'completed': False}, {'userId': 5, 'id': 83, 'title': 'quidem at rerum quis ex aut sit quam', 'completed': True}, {'userId': 5, 'id': 84, 'title': 'sunt veritatis ut voluptate', 'completed': False}, {'userId': 5, 'id': 85, 'title': 'et quia ad iste a', 'completed': True}, {'userId': 5, 'id': 86, 'title': 'incidunt ut saepe autem', 'completed': True}, {'userId': 5, 'id': 87, 'title': 'laudantium quae eligendi consequatur quia et vero autem', 'completed': True}, {'userId': 5, 'id': 88, 'title': 'vitae aut excepturi laboriosam sint aliquam et et accusantium', 'completed': False}, {'userId': 5, 'id': 89, 'title': 'sequi ut omnis et', 'completed': True}, {'userId': 5, 'id': 90, 'title': 'molestiae nisi accusantium tenetur dolorem et', 'completed': True}, {'userId': 5, 'id': 91, 'title': 'nulla quis consequatur saepe qui id expedita', 'completed': True}, {'userId': 5, 'id': 92, 'title': 'in omnis laboriosam', 'completed': True}, {'userId': 5, 'id': 93, 'title': 'odio iure consequatur molestiae quibusdam necessitatibus quia sint', 'completed': True}, {'userId': 5, 'id': 94, 'title': 'facilis modi saepe mollitia', 'completed': False}, {'userId': 5, 'id': 95, 'title': 'vel nihil et molestiae iusto assumenda nemo quo ut', 'completed': True}, {'userId': 5, 'id': 96, 'title': 'nobis suscipit ducimus enim asperiores voluptas', 'completed': False}, {'userId': 5, 'id': 97, 'title': 'dolorum laboriosam eos qui iure aliquam', 'completed': False}, {'userId': 5, 'id': 98, 'title': 'debitis accusantium ut quo facilis nihil quis sapiente necessitatibus', 'completed': True}, {'userId': 5, 'id': 99, 'title': 'neque voluptates ratione', 'completed': False}, {'userId': 5, 'id': 100, 'title': 'excepturi a et neque qui expedita vel voluptate', 'completed': False}, {'userId': 6, 'id': 101, 'title': 'explicabo enim cumque porro aperiam occaecati minima', 'completed': False}, {'userId': 6, 'id': 102, 'title': 'sed ab consequatur', 'completed': False}, {'userId': 6, 'id': 103, 'title': 'non sunt delectus illo nulla tenetur enim omnis', 'completed': False}, {'userId': 6, 'id': 104, 'title': 'excepturi non laudantium quo', 'completed': False}, {'userId': 6, 'id': 105, 'title': 'totam quia dolorem et illum repellat voluptas optio', 'completed': True}, {'userId': 6, 'id': 106, 'title': 'ad illo quis voluptatem temporibus', 'completed': True}, {'userId': 6, 'id': 107, 'title': 'praesentium facilis omnis laudantium fugit ad iusto nihil nesciunt', 'completed': False}, {'userId': 6, 'id': 108, 'title': 'a eos eaque nihil et exercitationem incidunt delectus', 'completed': True}, {'userId': 6, 'id': 109, 'title': 'autem temporibus harum quisquam in culpa', 'completed': True}, {'userId': 6, 'id': 110, 'title': 'aut aut ea corporis', 'completed': True}, {'userId': 6, 'id': 111, 'title': 'magni accusantium labore et id quis provident', 'completed': False}, {'userId': 6, 'id': 112, 'title': 'consectetur impedit quisquam qui deserunt non rerum consequuntur eius', 'completed': False}, {'userId': 6, 'id': 113, 'title': 'quia atque aliquam sunt impedit voluptatum rerum assumenda nisi', 'completed': False}, {'userId': 6, 'id': 114, 'title': 'cupiditate quos possimus corporis quisquam exercitationem beatae', 'completed': False}, {'userId': 6, 'id': 115, 'title': 'sed et ea eum', 'completed': False}, {'userId': 6, 'id': 116, 'title': 'ipsa dolores vel facilis ut', 'completed': True}, {'userId': 6, 'id': 117, 'title': 'sequi quae est et qui qui eveniet asperiores', 'completed': False}, {'userId': 6, 'id': 118, 'title': 'quia modi consequatur vero fugiat', 'completed': False}, {'userId': 6, 'id': 119, 'title': 'corporis ducimus ea perspiciatis iste', 'completed': False}, {'userId': 6, 'id': 120, 'title': 'dolorem laboriosam vel voluptas et aliquam quasi', 'completed': False}, {'userId': 7, 'id': 121, 'title': 'inventore aut nihil minima laudantium hic qui omnis', 'completed': True}, {'userId': 7, 'id': 122, 'title': 'provident aut nobis culpa', 'completed': True}, {'userId': 7, 'id': 123, 'title': 'esse et quis iste est earum aut impedit', 'completed': False}, {'userId': 7, 'id': 124, 'title': 'qui consectetur id', 'completed': False}, {'userId': 7, 'id': 125, 'title': 'aut quasi autem iste tempore illum possimus', 'completed': False}, {'userId': 7, 'id': 126, 'title': 'ut asperiores perspiciatis veniam ipsum rerum saepe', 'completed': True}, {'userId': 7, 'id': 127, 'title': 'voluptatem libero consectetur rerum ut', 'completed': True}, {'userId': 7, 'id': 128, 'title': 'eius omnis est qui voluptatem autem', 'completed': False}, {'userId': 7, 'id': 129, 'title': 'rerum culpa quis harum', 'completed': False}, {'userId': 7, 'id': 130, 'title': 'nulla aliquid eveniet harum laborum libero alias ut unde', 'completed': True}, {'userId': 7, 'id': 131, 'title': 'qui ea incidunt quis', 'completed': False}, {'userId': 7, 'id': 132, 'title': 'qui molestiae voluptatibus velit iure harum quisquam', 'completed': True}, {'userId': 7, 'id': 133, 'title': 'et labore eos enim rerum consequatur sunt', 'completed': True}, {'userId': 7, 'id': 134, 'title': 'molestiae doloribus et laborum quod ea', 'completed': False}, {'userId': 7, 'id': 135, 'title': 'facere ipsa nam eum voluptates reiciendis vero qui', 'completed': False}, {'userId': 7, 'id': 136, 'title': 'asperiores illo tempora fuga sed ut quasi adipisci', 'completed': False}, {'userId': 7, 'id': 137, 'title': 'qui sit non', 'completed': False}, {'userId': 7, 'id': 138, 'title': 'placeat minima consequatur rem qui ut', 'completed': True}, {'userId': 7, 'id': 139, 'title': 'consequatur doloribus id possimus voluptas a voluptatem', 'completed': False}, {'userId': 7, 'id': 140, 'title': 'aut consectetur in blanditiis deserunt quia sed laboriosam', 'completed': True}, {'userId': 8, 'id': 141, 'title': 'explicabo consectetur debitis voluptates quas quae culpa rerum non', 'completed': True}, {'userId': 8, 'id': 142, 'title': 'maiores accusantium architecto necessitatibus reiciendis ea aut', 'completed': True}, {'userId': 8, 'id': 143, 'title': 'eum non recusandae cupiditate animi', 'completed': False}, {'userId': 8, 'id': 144, 'title': 'ut eum exercitationem sint', 'completed': False}, {'userId': 8, 'id': 145, 'title': 'beatae qui ullam incidunt voluptatem non nisi aliquam', 'completed': False}, {'userId': 8, 'id': 146, 'title': 'molestiae suscipit ratione nihil odio libero impedit vero totam', 'completed': True}, {'userId': 8, 'id': 147, 'title': 'eum itaque quod reprehenderit et facilis dolor autem ut', 'completed': True}, {'userId': 8, 'id': 148, 'title': 'esse quas et quo quasi exercitationem', 'completed': False}, {'userId': 8, 'id': 149, 'title': 'animi voluptas quod perferendis est', 'completed': False}, {'userId': 8, 'id': 150, 'title': 'eos amet tempore laudantium fugit a', 'completed': False}, {'userId': 8, 'id': 151, 'title': 'accusamus adipisci dicta qui quo ea explicabo sed vero', 'completed': True}, {'userId': 8, 'id': 152, 'title': 'odit eligendi recusandae doloremque cumque non', 'completed': False}, {'userId': 8, 'id': 153, 'title': 'ea aperiam consequatur qui repellat eos', 'completed': False}, {'userId': 8, 'id': 154, 'title': 'rerum non ex sapiente', 'completed': True}, {'userId': 8, 'id': 155, 'title': 'voluptatem nobis consequatur et assumenda magnam', 'completed': True}, {'userId': 8, 'id': 156, 'title': 'nam quia quia nulla repellat assumenda quibusdam sit nobis', 'completed': True}, {'userId': 8, 'id': 157, 'title': 'dolorem veniam quisquam deserunt repellendus', 'completed': True}, {'userId': 8, 'id': 158, 'title': 'debitis vitae delectus et harum accusamus aut deleniti a', 'completed': True}, {'userId': 8, 'id': 159, 'title': 'debitis adipisci quibusdam aliquam sed dolore ea praesentium nobis', 'completed': True}, {'userId': 8, 'id': 160, 'title': 'et praesentium aliquam est', 'completed': False}, {'userId': 9, 'id': 161, 'title': 'ex hic consequuntur earum omnis alias ut occaecati culpa', 'completed': True}, {'userId': 9, 'id': 162, 'title': 'omnis laboriosam molestias animi sunt dolore', 'completed': True}, {'userId': 9, 'id': 163, 'title': 'natus corrupti maxime laudantium et voluptatem laboriosam odit', 'completed': False}, {'userId': 9, 'id': 164, 'title': 'reprehenderit quos aut aut consequatur est sed', 'completed': False}, {'userId': 9, 'id': 165, 'title': 'fugiat perferendis sed aut quidem', 'completed': False}, {'userId': 9, 'id': 166, 'title': 'quos quo possimus suscipit minima ut', 'completed': False}, {'userId': 9, 'id': 167, 'title': 'et quis minus quo a asperiores molestiae', 'completed': False}, {'userId': 9, 'id': 168, 'title': 'recusandae quia qui sunt libero', 'completed': False}, {'userId': 9, 'id': 169, 'title': 'ea odio perferendis officiis', 'completed': True}, {'userId': 9, 'id': 170, 'title': 'quisquam aliquam quia doloribus aut', 'completed': False}, {'userId': 9, 'id': 171, 'title': 'fugiat aut voluptatibus corrupti deleniti velit iste odio', 'completed': True}, {'userId': 9, 'id': 172, 'title': 'et provident amet rerum consectetur et voluptatum', 'completed': False}, {'userId': 9, 'id': 173, 'title': 'harum ad aperiam quis', 'completed': False}, {'userId': 9, 'id': 174, 'title': 'similique aut quo', 'completed': False}, {'userId': 9, 'id': 175, 'title': 'laudantium eius officia perferendis provident perspiciatis asperiores', 'completed': True}, {'userId': 9, 'id': 176, 'title': 'magni soluta corrupti ut maiores rem quidem', 'completed': False}, {'userId': 9, 'id': 177, 'title': 'et placeat temporibus voluptas est tempora quos quibusdam', 'completed': False}, {'userId': 9, 'id': 178, 'title': 'nesciunt itaque commodi tempore', 'completed': True}, {'userId': 9, 'id': 179, 'title': 'omnis consequuntur cupiditate impedit itaque ipsam quo', 'completed': True}, {'userId': 9, 'id': 180, 'title': 'debitis nisi et dolorem repellat et', 'completed': True}, {'userId': 10, 'id': 181, 'title': 'ut cupiditate sequi aliquam fuga maiores', 'completed': False}, {'userId': 10, 'id': 182, 'title': 'inventore saepe cumque et aut illum enim', 'completed': True}, {'userId': 10, 'id': 183, 'title': 'omnis nulla eum aliquam distinctio', 'completed': True}, {'userId': 10, 'id': 184, 'title': 'molestias modi perferendis perspiciatis', 'completed': False}, {'userId': 10, 'id': 185, 'title': 'voluptates dignissimos sed doloribus animi quaerat aut', 'completed': False}, {'userId': 10, 'id': 186, 'title': 'explicabo odio est et', 'completed': False}, {'userId': 10, 'id': 187, 'title': 'consequuntur animi possimus', 'completed': False}, {'userId': 10, 'id': 188, 'title': 'vel non beatae est', 'completed': True}, {'userId': 10, 'id': 189, 'title': 'culpa eius et voluptatem et', 'completed': True}, {'userId': 10, 'id': 190, 'title': 'accusamus sint iusto et voluptatem exercitationem', 'completed': True}, {'userId': 10, 'id': 191, 'title': 'temporibus atque distinctio omnis eius impedit tempore molestias pariatur', 'completed': True}, {'userId': 10, 'id': 192, 'title': 'ut quas possimus exercitationem sint voluptates', 'completed': False}, {'userId': 10, 'id': 193, 'title': 'rerum debitis voluptatem qui eveniet tempora distinctio a', 'completed': True}, {'userId': 10, 'id': 194, 'title': 'sed ut vero sit molestiae', 'completed': False}, {'userId': 10, 'id': 195, 'title': 'rerum ex veniam mollitia voluptatibus pariatur', 'completed': True}, {'userId': 10, 'id': 196, 'title': 'consequuntur aut ut fugit similique', 'completed': True}, {'userId': 10, 'id': 197, 'title': 'dignissimos quo nobis earum saepe', 'completed': True}, {'userId': 10, 'id': 198, 'title': 'quis eius est sint explicabo', 'completed': True}, {'userId': 10, 'id': 199, 'title': 'numquam repellendus a magnam', 'completed': True}, {'userId': 10, 'id': 200, 'title': 'ipsam aperiam voluptates qui', 'completed': False}]\n"
     ]
    }
   ],
   "source": [
    "# Fetch data dari API\n",
    "r = requests.get(\"https://jsonplaceholder.typicode.com/todos\")\n",
    "res = r.json()\n",
    "\n",
    "# Cetak hasil fetch data dari API\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setelah mendapatkan data JSON, kita dapat mengekstrak informasi spesifik dari struktur data tersebut, seperti kutipan dan penulis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'delectus aut autem'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mengekstrak kutipan dari data JSON\n",
    "q = res[0]['title']\n",
    "q\n",
    "# Menampilkan kutipan dan penulis\n",
    "# print(q['quote'], '\\n--', q['author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Membaca text dari file HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/lib/python3.13/site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/lib/python3.13/site-packages (from beautifulsoup4) (2.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup   \n",
    "import urllib.request as urllib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = urllib2.urlopen('https://en.wikipedia.org/wiki/Natural_language_processing')\n",
    "html_doc = response.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Natural language processing - Wikipedia</title>\n",
      "Natural language processing - Wikipedia\n",
      "Jump to content\n",
      "None\n",
      "Jump to content\n",
      "Main page\n",
      "Contents\n",
      "Current events\n",
      "Random article\n",
      "About Wikipedia\n",
      "Contact us\n",
      "Help\n",
      "Learn to edit\n",
      "Community portal\n",
      "Recent changes\n",
      "Upload file\n",
      "None\n",
      "None\n",
      "Donate\n",
      "Create account\n",
      "Log in\n",
      "Donate\n",
      "None\n",
      "None\n",
      "learn more\n",
      "Contributions\n",
      "Talk\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Afrikaans\n",
      "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\n",
      "Ô±Ö€Õ¥Ö‚Õ´Õ¿Õ¡Õ°Õ¡ÕµÕ¥Ö€Õ§Õ¶\n",
      "AzÉ™rbaycanca\n",
      "à¦¬à¦¾à¦‚à¦²à¦¾\n",
      "é–©å—èª / BÃ¢n-lÃ¢m-gÃº\n",
      "Ğ‘ĞµĞ»Ğ°Ñ€ÑƒÑĞºĞ°Ñ\n",
      "Ğ‘ĞµĞ»Ğ°Ñ€ÑƒÑĞºĞ°Ñ (Ñ‚Ğ°Ñ€Ğ°ÑˆĞºĞµĞ²Ñ–Ñ†Ğ°)\n",
      "Ğ‘ÑŠĞ»Ğ³Ğ°Ñ€ÑĞºĞ¸\n",
      "Bosanski\n",
      "Brezhoneg\n",
      "CatalÃ \n",
      "ÄŒeÅ¡tina\n",
      "Cymraeg\n",
      "Dansk\n",
      "Deutsch\n",
      "Eesti\n",
      "Î•Î»Î»Î·Î½Î¹ÎºÎ¬\n",
      "EspaÃ±ol\n",
      "Esperanto\n",
      "Euskara\n",
      "ÙØ§Ø±Ø³ÛŒ\n",
      "FranÃ§ais\n",
      "Gaeilge\n",
      "Galego\n",
      "í•œêµ­ì–´\n",
      "Õ€Õ¡ÕµÕ¥Ö€Õ¥Õ¶\n",
      "à¤¹à¤¿à¤¨à¥à¤¦à¥€\n",
      "Hrvatski\n",
      "Bahasa Indonesia\n",
      "IsiZulu\n",
      "Ãslenska\n",
      "Italiano\n",
      "×¢×‘×¨×™×ª\n",
      "à²•à²¨à³à²¨à²¡\n",
      "áƒ¥áƒáƒ áƒ—áƒ£áƒšáƒ˜\n",
      "LatvieÅ¡u\n",
      "LietuviÅ³\n",
      "ĞœĞ°ĞºĞµĞ´Ğ¾Ğ½ÑĞºĞ¸\n",
      "à¤®à¤°à¤¾à¤ à¥€\n",
      "Ù…ØµØ±Ù‰\n",
      "ĞœĞ¾Ğ½Ğ³Ğ¾Ğ»\n",
      "á€™á€¼á€”á€ºá€™á€¬á€˜á€¬á€á€¬\n",
      "Nederlands\n",
      "æ—¥æœ¬èª\n",
      "à¬“à¬¡à¬¼à¬¿à¬†\n",
      "Ù¾ÚšØªÙˆ\n",
      "Picard\n",
      "PiemontÃ¨is\n",
      "Polski\n",
      "PortuguÃªs\n",
      "Qaraqalpaqsha\n",
      "RomÃ¢nÄƒ\n",
      "Runa Simi\n",
      "Ğ ÑƒÑÑĞºĞ¸Ğ¹\n",
      "Shqip\n",
      "Simple English\n",
      "Ú©ÙˆØ±Ø¯ÛŒ\n",
      "Ğ¡Ñ€Ğ¿ÑĞºĞ¸ / srpski\n",
      "Srpskohrvatski / ÑÑ€Ğ¿ÑĞºĞ¾Ñ…Ñ€Ğ²Ğ°Ñ‚ÑĞºĞ¸\n",
      "Suomi\n",
      "à®¤à®®à®¿à®´à¯\n",
      "à°¤à±†à°²à±à°—à±\n",
      "à¹„à¸—à¸¢\n",
      "TÃ¼rkÃ§e\n",
      "Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°\n",
      "Tiáº¿ng Viá»‡t\n",
      "ç²µèª\n",
      "ä¸­æ–‡\n",
      "Edit links\n",
      "Article\n",
      "Talk\n",
      "Read\n",
      "Edit\n",
      "View history\n",
      "Read\n",
      "Edit\n",
      "View history\n",
      "What links here\n",
      "Related changes\n",
      "Upload file\n",
      "Special pages\n",
      "Permanent link\n",
      "Page information\n",
      "Cite this page\n",
      "Get shortened URL\n",
      "Download QR code\n",
      "Download as PDF\n",
      "Printable version\n",
      "Wikimedia Commons\n",
      "Wikiversity\n",
      "Wikidata item\n",
      "NLP\n",
      "Language processing in the brain\n",
      "None\n",
      "verification\n",
      "improve this article\n",
      "adding citations to reliable sources\n",
      "\"Natural language processing\"\n",
      "news\n",
      "newspapers\n",
      "books\n",
      "scholar\n",
      "JSTOR\n",
      "Learn how and when to remove this message\n",
      "computer science\n",
      "artificial intelligence\n",
      "natural language\n",
      "information retrieval\n",
      "knowledge representation\n",
      "computational linguistics\n",
      "linguistics\n",
      "text corpora\n",
      "machine learning\n",
      "deep learning\n",
      "speech recognition\n",
      "text classification\n",
      "natural-language understanding\n",
      "natural-language generation\n",
      "edit\n",
      "History of natural language processing\n",
      "None\n",
      "Alan Turing\n",
      "Computing Machinery and Intelligence\n",
      "Turing test\n",
      "edit\n",
      "John Searle\n",
      "Chinese room\n",
      "Georgetown experiment\n",
      "automatic translation\n",
      "None\n",
      "ALPAC report\n",
      "None\n",
      "statistical machine translation\n",
      "SHRDLU\n",
      "blocks worlds\n",
      "ELIZA\n",
      "Rogerian psychotherapist\n",
      "Joseph Weizenbaum\n",
      "Ross Quillian\n",
      "None\n",
      "ontologies\n",
      "chatterbots\n",
      "PARRY\n",
      "HPSG\n",
      "generative grammar\n",
      "None\n",
      "Lesk algorithm\n",
      "None\n",
      "Rhetorical Structure Theory\n",
      "Racter\n",
      "Jabberwacky\n",
      "None\n",
      "edit\n",
      "machine learning\n",
      "Moore's law\n",
      "Chomskyan\n",
      "transformational grammar\n",
      "corpus linguistics\n",
      "None\n",
      "machine translation\n",
      "IBM alignment models\n",
      "textual corpora\n",
      "Parliament of Canada\n",
      "European Union\n",
      "unsupervised\n",
      "semi-supervised learning\n",
      "supervised learning\n",
      "World Wide Web\n",
      "time complexity\n",
      "word n-gram model\n",
      "multi-layer perceptron\n",
      "context length\n",
      "Bengio\n",
      "None\n",
      "TomÃ¡Å¡ Mikolov\n",
      "Brno University of Technology\n",
      "recurrent neural network\n",
      "None\n",
      "Word2vec\n",
      "representation learning\n",
      "deep neural network\n",
      "None\n",
      "None\n",
      "language modeling\n",
      "None\n",
      "None\n",
      "None\n",
      "in medicine and healthcare\n",
      "electronic health records\n",
      "None\n",
      "None\n",
      "edit\n",
      "None\n",
      "None\n",
      "stemming\n",
      "Machine learning\n",
      "language models\n",
      "intractability\n",
      "LLMs\n",
      "Apertium\n",
      "tokenization\n",
      "knowledge extraction\n",
      "edit\n",
      "AI winter\n",
      "None\n",
      "None\n",
      "decision trees\n",
      "ifâ€“then rules\n",
      "Markov models\n",
      "edit\n",
      "Artificial neural network\n",
      "feature engineering\n",
      "None\n",
      "neural networks\n",
      "semantic networks\n",
      "None\n",
      "word embeddings\n",
      "Neural machine translation\n",
      "sequence-to-sequence\n",
      "statistical machine translation\n",
      "edit\n",
      "edit\n",
      "Optical character recognition\n",
      "Speech recognition\n",
      "text to speech\n",
      "AI-complete\n",
      "natural speech\n",
      "speech segmentation\n",
      "coarticulation\n",
      "analog signal\n",
      "Speech segmentation\n",
      "speech recognition\n",
      "Text-to-speech\n",
      "None\n",
      "Word segmentation\n",
      "Tokenization\n",
      "None\n",
      "English\n",
      "Chinese\n",
      "Japanese\n",
      "Thai\n",
      "vocabulary\n",
      "morphology\n",
      "bag of words\n",
      "citation needed\n",
      "edit\n",
      "Lemmatization\n",
      "None\n",
      "Morphological segmentation\n",
      "morphemes\n",
      "morphology\n",
      "English\n",
      "inflectional morphology\n",
      "Turkish\n",
      "Meitei\n",
      "agglutinated\n",
      "None\n",
      "Part-of-speech tagging\n",
      "part of speech\n",
      "noun\n",
      "verb\n",
      "adjective\n",
      "Stemming\n",
      "edit\n",
      "a series\n",
      "Formal languages\n",
      "Formal system\n",
      "Alphabet\n",
      "Syntax\n",
      "Formal semantics\n",
      "Semantics (programming languages)\n",
      "Formal grammar\n",
      "Formation rule\n",
      "Well-formed formula\n",
      "Automata theory\n",
      "Regular expression\n",
      "Production\n",
      "Ground expression\n",
      "Atomic formula\n",
      "Formal methods\n",
      "Propositional calculus\n",
      "Predicate logic\n",
      "Mathematical notation\n",
      "Natural language processing\n",
      "Programming language theory\n",
      "Computational linguistics\n",
      "Syntax analysis\n",
      "Formal verification\n",
      "Automated theorem proving\n",
      "v\n",
      "t\n",
      "e\n",
      "Grammar induction\n",
      "None\n",
      "formal grammar\n",
      "Sentence breaking\n",
      "sentence boundary disambiguation\n",
      "periods\n",
      "punctuation marks\n",
      "abbreviations\n",
      "Parsing\n",
      "parse tree\n",
      "grammar\n",
      "natural languages\n",
      "ambiguous\n",
      "probabilistic context-free grammar\n",
      "stochastic grammar\n",
      "edit\n",
      "Lexical semantics\n",
      "Distributional semantics\n",
      "Named entity recognition\n",
      "capitalization\n",
      "named entity\n",
      "Chinese\n",
      "Arabic\n",
      "German\n",
      "nouns\n",
      "French\n",
      "Spanish\n",
      "adjectives\n",
      "None\n",
      "Sentiment analysis\n",
      "Multimodal sentiment analysis\n",
      "word n-grams\n",
      "Term Frequency-Inverse Document Frequency\n",
      "deep learning\n",
      "None\n",
      "Terminology extraction\n",
      "Word-sense disambiguation\n",
      "meaning\n",
      "WordNet\n",
      "Entity linking\n",
      "named entities\n",
      "edit\n",
      "Relationship extraction\n",
      "Semantic parsing\n",
      "AMR parsing\n",
      "DRT parsing\n",
      "Natural language understanding\n",
      "Semantic role labelling\n",
      "frames\n",
      "semantic roles\n",
      "edit\n",
      "Coreference resolution\n",
      "Anaphora resolution\n",
      "pronouns\n",
      "referring expressions\n",
      "Discourse analysis\n",
      "discourse\n",
      "speech acts\n",
      "frames\n",
      "Semantic role labelling\n",
      "pro-drop languages\n",
      "Recognizing textual entailment\n",
      "None\n",
      "Topic segmentation\n",
      "Argument mining\n",
      "natural language\n",
      "None\n",
      "argument scheme\n",
      "None\n",
      "None\n",
      "edit\n",
      "Automatic summarization\n",
      "None\n",
      "None\n",
      "None\n",
      "GPT-2\n",
      "Logic translation\n",
      "Machine translation\n",
      "AI-complete\n",
      "Natural-language understanding\n",
      "first-order logic\n",
      "computer\n",
      "closed-world assumption\n",
      "open-world assumption\n",
      "None\n",
      "Natural-language generation\n",
      "None\n",
      "1 the Road\n",
      "language models\n",
      "None\n",
      "Document AI\n",
      "None\n",
      "Dialogue management\n",
      "Question answering\n",
      "Text-to-image generation\n",
      "None\n",
      "3D model\n",
      "None\n",
      "None\n",
      "Text-to-video\n",
      "None\n",
      "None\n",
      "edit\n",
      "None\n",
      "edit\n",
      "Cognition\n",
      "None\n",
      "Cognitive science\n",
      "None\n",
      "Cognitive linguistics\n",
      "None\n",
      "symbolic NLP\n",
      "George Lakoff\n",
      "None\n",
      "conceptual metaphor\n",
      "None\n",
      "probabilistic context-free grammar\n",
      "US Patent 9269353\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "ACT-R\n",
      "None\n",
      "ACL\n",
      "explainability\n",
      "None\n",
      "multimodal\n",
      "None\n",
      "artificial intelligence\n",
      "large language model\n",
      "None\n",
      "artificial general intelligence\n",
      "free energy principle\n",
      "None\n",
      "Karl J. Friston\n",
      "edit\n",
      "1 the Road\n",
      "Artificial intelligence detection software\n",
      "Automated essay scoring\n",
      "Biomedical text mining\n",
      "Compound term processing\n",
      "Computational linguistics\n",
      "Computer-assisted reviewing\n",
      "Controlled natural language\n",
      "Deep learning\n",
      "Deep linguistic processing\n",
      "Distributional semantics\n",
      "Foreign language reading aid\n",
      "Foreign language writing aid\n",
      "Information extraction\n",
      "Information retrieval\n",
      "Language and Communication Technologies\n",
      "Language model\n",
      "Language technology\n",
      "Latent semantic indexing\n",
      "Multi-agent system\n",
      "Native-language identification\n",
      "Natural-language programming\n",
      "Natural-language understanding\n",
      "Natural-language search\n",
      "Outline of natural language processing\n",
      "Query expansion\n",
      "Query understanding\n",
      "Reification (linguistics)\n",
      "Speech processing\n",
      "Spoken dialogue systems\n",
      "Text-proofing\n",
      "Text simplification\n",
      "Transformer (machine learning model)\n",
      "Truecasing\n",
      "Question answering\n",
      "Word2vec\n",
      "edit\n",
      "^\n",
      "\"NLP\"\n",
      "^\n",
      "\"The history of machine translation in a nutshell\"\n",
      "self-published source\n",
      "^\n",
      "^\n",
      "Crevier 1993\n",
      "help\n",
      "Buchanan 2005\n",
      "help\n",
      "^\n",
      "Koskenniemi, Kimmo\n",
      "Two-level morphology: A general computational model of word-form recognition and production\n",
      "University of Helsinki\n",
      "^\n",
      "Control of Inference: Role of Some Aspects of Discourse Structure-Centering\n",
      "^\n",
      "doi\n",
      "10.1109/PROC.1986.13580\n",
      "ISSN\n",
      "1558-2256\n",
      "S2CID\n",
      "30688575\n",
      "^\n",
      "corner cases\n",
      "pathological\n",
      "thought experiments\n",
      "corpus linguistics\n",
      "corpora\n",
      "poverty of the stimulus\n",
      "^\n",
      "\"A neural probabilistic language model\"\n",
      "^\n",
      "\"Recurrent neural network based language model\"\n",
      "doi\n",
      "10.21437/Interspeech.2010-343\n",
      "S2CID\n",
      "17048224\n",
      "cite book\n",
      "help\n",
      "^\n",
      "arXiv\n",
      "1807.10854\n",
      "doi\n",
      "10.1613/jair.4992\n",
      "S2CID\n",
      "8273530\n",
      "^\n",
      "Deep Learning\n",
      "^\n",
      "arXiv\n",
      "1602.02410\n",
      "Bibcode\n",
      "2016arXiv160202410J\n",
      "^\n",
      "\"Parsing as Language Modeling\"\n",
      "the original\n",
      "^\n",
      "\"Grammar as a Foreign Language\"\n",
      "arXiv\n",
      "1412.7449\n",
      "Bibcode\n",
      "2014arXiv1412.7449V\n",
      "^\n",
      "\"Using Natural Language Processing to Measure and Improve Quality of Diabetes Care: A Systematic Review\"\n",
      "doi\n",
      "10.1177/19322968211000831\n",
      "ISSN\n",
      "1932-2968\n",
      "PMC\n",
      "8120048\n",
      "PMID\n",
      "33736486\n",
      "^\n",
      "\"Prevalence of Sensitive Terms in Clinical Notes Using Natural Language Processing Techniques: Observational Study\"\n",
      "doi\n",
      "10.2196/38482\n",
      "ISSN\n",
      "2291-9694\n",
      "PMC\n",
      "9233261\n",
      "PMID\n",
      "35687381\n",
      "^\n",
      "Procedures as a Representation for Data in a Computer Program for Understanding Natural Language\n",
      "^\n",
      "ISBN\n",
      "0-470-99033-3\n",
      "^\n",
      "Mark Johnson. How the statistical revolution changes (computational) linguistics.\n",
      "^\n",
      "Philip Resnik. Four revolutions.\n",
      "^\n",
      "\"Deep Learning For NLP-ACL 2012 Tutorial\"\n",
      "http://web.stanford.edu/class/cs224n/\n",
      "^\n",
      "Semantic Network Analysis in Social Sciences\n",
      "ISBN\n",
      "9780367636524\n",
      "Archived\n",
      "^\n",
      "Tian, Yingli\n",
      "CiteSeerX\n",
      "10.1.1.668.869\n",
      "doi\n",
      "10.1007/978-3-642-29364-1_2\n",
      "ISBN\n",
      "9783642293634\n",
      "a\n",
      "b\n",
      "\"Natural Language Processing (NLP) - A Complete Guide\"\n",
      "^\n",
      "\"What is Natural Language Processing? Intro to NLP in Machine Learning\"\n",
      "^\n",
      "\"Manipuri Morpheme Identification\"\n",
      "cite journal\n",
      "link\n",
      "^\n",
      "\"Natural language grammar induction using a constituent-context model\"\n",
      "^\n",
      "\"Precision information extraction for rare disease epidemiology at scale\"\n",
      "doi\n",
      "10.1186/s12967-023-04011-y\n",
      "PMC\n",
      "9972634\n",
      "PMID\n",
      "36855134\n",
      "^\n",
      "https://tac.nist.gov//2011/RTE/\n",
      "^\n",
      "\"Argumentation Mining: State of the Art and Emerging Trends\"\n",
      "doi\n",
      "10.1145/2850417\n",
      "hdl\n",
      "11585/523460\n",
      "ISSN\n",
      "1533-5399\n",
      "S2CID\n",
      "9561587\n",
      "^\n",
      "\"Argument Mining â€“ IJCAI2016 Tutorial\"\n",
      "^\n",
      "\"NLP Approaches to Computational Argumentation â€“ ACL 2016, Berlin\"\n",
      "^\n",
      "\"Centre for Language Technology (CLT)\"\n",
      "^\n",
      "\"Shared Task: Grammatical Error Correction\"\n",
      "^\n",
      "\"Shared Task: Grammatical Error Correction\"\n",
      "^\n",
      "\"Formalizing Semantic of Natural Language through Conceptualization from Existence\"\n",
      "the original\n",
      "^\n",
      "\"U B U W E BÂ :: Racter\"\n",
      "^\n",
      "doi\n",
      "10.1007/978-3-030-16800-1\n",
      "ISBN\n",
      "978-3-030-16799-8\n",
      "S2CID\n",
      "155818532\n",
      "^\n",
      "\"Document Understanding AI on Google Cloud (Cloud Next '19) â€“ YouTube\"\n",
      "the original\n",
      "^\n",
      "\"OpenAI's DALL-E AI image generator can now edit pictures, too\"\n",
      "^\n",
      "\"The Stanford Natural Language Processing Group\"\n",
      "^\n",
      "\"WordsEye\"\n",
      "doi\n",
      "10.1145/383259.383316\n",
      "ISBN\n",
      "978-1-58113-374-5\n",
      "S2CID\n",
      "3842372\n",
      "^\n",
      "\"Google announces AI advances in text-to-video, language translation, more\"\n",
      "^\n",
      "\"Meta's new text-to-video AI generator is like DALL-E for video\"\n",
      "^\n",
      "\"Previous shared tasks | CoNLL\"\n",
      "^\n",
      "\"Cognition\"\n",
      "Oxford University Press\n",
      "Dictionary.com\n",
      "the original\n",
      "^\n",
      "\"Ask the Cognitive Scientist\"\n",
      "^\n",
      "ISBN\n",
      "978-0-805-85352-0\n",
      "^\n",
      "ISBN\n",
      "978-0-465-05674-3\n",
      "^\n",
      "ISBN\n",
      "978-0-521-59541-4\n",
      "^\n",
      "US patent 9269353\n",
      "^\n",
      "\"Universal Conceptual Cognitive Annotation (UCCA)\"\n",
      "^\n",
      "Building an RRG computational grammar\n",
      "^\n",
      "\"Fluid Construction Grammar â€“ A fully operational processing system for construction grammars\"\n",
      "^\n",
      "\"ACL Member Portal | The Association for Computational Linguistics Member Portal\"\n",
      "^\n",
      "\"Chunks and Rules\"\n",
      "^\n",
      "\"Grounded Compositional Semantics for Finding and Describing Images with Sentences\"\n",
      "doi\n",
      "10.1162/tacl_a_00177\n",
      "S2CID\n",
      "2317858\n",
      "^\n",
      "arXiv\n",
      "2207.07051\n",
      "cs.CL\n",
      "^\n",
      "ISBN\n",
      "978-0-262-36997-8\n",
      "edit\n",
      "\"Models of natural language understanding\"\n",
      "Bibcode\n",
      "1995PNAS...92.9977B\n",
      "doi\n",
      "10.1073/pnas.92.22.9977\n",
      "PMC\n",
      "40721\n",
      "PMID\n",
      "7479812\n",
      "ISBN\n",
      "978-0-596-51649-9\n",
      "Kenna Hughes-Castleberry\n",
      "Cain's Jawbone\n",
      "Scientific American\n",
      "natural-language processing\n",
      "context\n",
      "ancient languages\n",
      "civilizations\n",
      "training data\n",
      "ISBN\n",
      "978-0-13-187321-6\n",
      "ISBN\n",
      "978-1848218482\n",
      "ISBN\n",
      "978-1848219212\n",
      "ISBN\n",
      "978-0-521-86571-5\n",
      "Official html and pdf versions available without charge.\n",
      "ISBN\n",
      "978-0-262-13360-9\n",
      "ISBN\n",
      "978-0-387-19557-5\n",
      "edit\n",
      "None\n",
      "Natural language processing\n",
      "v\n",
      "t\n",
      "e\n",
      "Natural language processing\n",
      "AI-complete\n",
      "Bag-of-words\n",
      "n-gram\n",
      "Bigram\n",
      "Trigram\n",
      "Computational linguistics\n",
      "Natural language understanding\n",
      "Stop words\n",
      "Text processing\n",
      "Text analysis\n",
      "Argument mining\n",
      "Collocation extraction\n",
      "Concept mining\n",
      "Coreference resolution\n",
      "Deep linguistic processing\n",
      "Distant reading\n",
      "Information extraction\n",
      "Named-entity recognition\n",
      "Ontology learning\n",
      "Parsing\n",
      "Semantic parsing\n",
      "Syntactic parsing\n",
      "Part-of-speech tagging\n",
      "Semantic analysis\n",
      "Semantic role labeling\n",
      "Semantic decomposition\n",
      "Semantic similarity\n",
      "Sentiment analysis\n",
      "Terminology extraction\n",
      "Text mining\n",
      "Textual entailment\n",
      "Truecasing\n",
      "Word-sense disambiguation\n",
      "Word-sense induction\n",
      "Text segmentation\n",
      "Compound-term processing\n",
      "Lemmatisation\n",
      "Lexical analysis\n",
      "Text chunking\n",
      "Stemming\n",
      "Sentence segmentation\n",
      "Word segmentation\n",
      "Automatic summarization\n",
      "Multi-document summarization\n",
      "Sentence extraction\n",
      "Text simplification\n",
      "Machine translation\n",
      "Computer-assisted\n",
      "Example-based\n",
      "Rule-based\n",
      "Statistical\n",
      "Transfer-based\n",
      "Neural\n",
      "Distributional semantics\n",
      "BERT\n",
      "Document-term matrix\n",
      "Explicit semantic analysis\n",
      "fastText\n",
      "GloVe\n",
      "Language model\n",
      "large\n",
      "Latent semantic analysis\n",
      "Seq2seq\n",
      "Word embedding\n",
      "Word2vec\n",
      "Language resources\n",
      "Corpus linguistics\n",
      "Lexical resource\n",
      "Linguistic Linked Open Data\n",
      "Machine-readable dictionary\n",
      "Parallel text\n",
      "PropBank\n",
      "Semantic network\n",
      "Simple Knowledge Organization System\n",
      "Speech corpus\n",
      "Text corpus\n",
      "Thesaurus (information retrieval)\n",
      "Treebank\n",
      "Universal Dependencies\n",
      "BabelNet\n",
      "Bank of English\n",
      "DBpedia\n",
      "FrameNet\n",
      "Google Ngram Viewer\n",
      "UBY\n",
      "WordNet\n",
      "Wikidata\n",
      "None\n",
      "Speech recognition\n",
      "Speech segmentation\n",
      "Speech synthesis\n",
      "Natural language generation\n",
      "Optical character recognition\n",
      "Topic model\n",
      "Document classification\n",
      "Latent Dirichlet allocation\n",
      "Pachinko allocation\n",
      "None\n",
      "Automated essay scoring\n",
      "Concordancer\n",
      "Grammar checker\n",
      "Predictive text\n",
      "Pronunciation assessment\n",
      "Spell checker\n",
      "None\n",
      "Chatbot\n",
      "Interactive fiction\n",
      "Syntax guessing\n",
      "Question answering\n",
      "Virtual assistant\n",
      "Voice user interface\n",
      "Formal semantics\n",
      "Hallucination\n",
      "Natural Language Toolkit\n",
      "spaCy\n",
      "Portal\n",
      "None\n",
      "Language\n",
      "Authority control databases\n",
      "None\n",
      "United States\n",
      "Japan\n",
      "Czech Republic\n",
      "Israel\n",
      "https://en.wikipedia.org/w/index.php?title=Natural_language_processing&oldid=1274942014\n",
      "Categories\n",
      "Natural language processing\n",
      "Computational fields of study\n",
      "Computational linguistics\n",
      "Speech recognition\n",
      "All accuracy disputes\n",
      "Accuracy disputes from December 2013\n",
      "Harv and Sfn no-target errors\n",
      "CS1 errors: periodical ignored\n",
      "CS1 maint: location\n",
      "Articles with short description\n",
      "Short description is different from Wikidata\n",
      "Articles needing additional references from May 2024\n",
      "All articles needing additional references\n",
      "All articles with unsourced statements\n",
      "Articles with unsourced statements from May 2024\n",
      "Commons category link from Wikidata\n",
      "Creative Commons Attribution-ShareAlike 4.0 License\n",
      "Terms of Use\n",
      "Privacy Policy\n",
      "Wikimedia Foundation, Inc.\n",
      "Privacy policy\n",
      "About Wikipedia\n",
      "Disclaimers\n",
      "Contact Wikipedia\n",
      "Code of Conduct\n",
      "Developers\n",
      "Statistics\n",
      "Cookie statement\n",
      "Mobile view\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Parsing\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "# Formating the parsed html file\n",
    "strhtm = soup.prettify()\n",
    "\n",
    "# Mengambil data\n",
    "print(soup.title)\n",
    "print(soup.title.string)\n",
    "print(soup.a.string)\n",
    "print(soup.b.string)\n",
    "\n",
    "# Mengambil semua tag tertentu\n",
    "for x in soup.find_all('a'): print(x.string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
